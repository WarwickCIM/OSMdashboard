---
title: "Group contributions to OSM"
format: 
  dashboard:
    # orientation: columns
    orientation: rows
    # scrolling: true
    self-contained: true
    nav-buttons:
        - icon: github
          href: https://github.com/WarwickCIM/OSMdashboard
          
params:
  use_db_overlay: false #this allows the database to be used to populate the .csv files
  db_path: "database/osm_changesets.duckdb"  #please change if the DB is somewhere else
  hashtags: ""  #formatted as "#HOT, #MissingMaps"
  usernames: ""# formatted as "alice123,bob_mapper"
  start: "" # formatted as "2023-01-01"
  end: ""   # formatted as "2023-01-01"
  bbox: "" #formatted a: "min_lat,min_lon,max_lat,max_lon" e.g "51.2,-0.5,51.8,0.3"     
---

```{r user input}
# Uncomment to update datasets.
# source(file = "data_retrieval.R")

# Add folder name if dashboard is not in the root of the project. Add trailing /
base_path <- ""

```


```{r setup}
#| echo: false
#| warning: false
#| output: false

library(dplyr)
library(forcats)
library(ggplot2)
library(here)
library(knitr)
library(leaflet)
library(leaflet.extras)
library(lubridate)
library(OSMdashboard)
library(plotly)
library(readr)
library(sf)
library(stringr)
library(tidyr)
library(treemapify)
library(wordcloud)

# GGplot configs --------------------------------------------------------------

# theme_set(
#   theme_minimal(
#     base_size = 14) +
#   theme(
#     plot.title = element_text(face = "bold"),
#     plot.title.position = "plot",
#     legend.position = "top"
#   )
# )

# Heatmap theme for Changeset and WikiEdit distribution
theme_heatmap <- theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.title = element_text(
      family = "Arial",
      face = "bold",
      size = 16,
      hjust = 0.5
    ),
    axis.text.x = element_text(family = "Arial", size = 8,
                               angle = 45, vjust = 0.5, hjust = 1),
    axis.text.y = element_text(family = "Arial", size = 8),
    legend.position = "none"
  )

theme_user_stats <- theme_minimal() +
  theme(panel.grid = element_blank(),
        axis.line = element_line(colour = 'grey20', size = 0.5),
        legend.position = "none",
        plot.margin = margin(5,20,5,20))

## Colour Scheme: https://colorbrewer2.org/#type=diverging&scheme=PuOr&n=4

# Colours for marks
colour_primary_mark <- "#e66101"
colour_secondary_mark <- "#5e3c99"
colour_tertiary_mark <- "#2d004b"
colour_fill <- "#fdb863"

```

```{python}
#| label: db-overlay
#| echo: false
#| cache: true
#| message: false 
#| warning: false
# Creates data/db_overlay/* from DuckDB for the current group.
#we add the cache:true in order to cache the overlay chunk so it doesn't re-run unless inputs change

import os, sys, subprocess, csv

def ensure(pkg):
    try: __import__(pkg)
    except ImportError: subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])

for p in ("duckdb","pandas","pyarrow"): ensure(p)

import pandas as pd
import duckdb

#locate the database files at database/osm_changesets.duckdb and python.queries.py file
DB_PATH = os.getenv("DUCKDB_PATH", "../database/osm_changesets.duckdb")
QUERIES_PATH = "../python/queries.py"  # adjust if you put it elsewhere

if not os.path.exists(DB_PATH):
    raise RuntimeError(f"DuckDB not found at {DB_PATH}. Put the file in database/osm_changesets.duckdb or set DUCKDB_PATH.")

#impor the relevant helper functions
import importlib.util
spec = importlib.util.spec_from_file_location("queries", QUERIES_PATH)
if spec is None or spec.loader is None:
    raise RuntimeError(f"Cannot import queries.py at {QUERIES_PATH}.")
queries = importlib.util.module_from_spec(spec); spec.loader.exec_module(queries)

#get group usernames from the repo
# expects data/metadata/group_users.csv with column username
users_csv = "data/metadata/group_users.csv"
if not os.path.exists(users_csv):
    raise RuntimeError("Expected group users at data/metadata/group_users.csv (with column 'username' or 'user').")

uinfo = pd.read_csv(users_csv)
name_col = "username" if "username" in uinfo.columns else ("user" if "user" in uinfo.columns else None)
if name_col is None:
    raise RuntimeError("user_info.csv must have a 'username' or 'user' column.")
usernames = sorted({str(x).strip() for x in uinfo[name_col].dropna().tolist() if str(x).strip()})

if not usernames:
    raise RuntimeError("No usernames found in data/metadata/user_info.csv.")

#optionally adda timeframe for the group
start = end = None
try:
    gpath = "data/metadata/group_info.csv"
    if os.path.exists(gpath):
        gdf = pd.read_csv(gpath)
        for col in ("start","start_date","from"):
            if col in gdf.columns and pd.notna(gdf[col].iloc[0]): start = str(gdf[col].iloc[0])
        for col in ("end","end_date","to"):
            if col in gdf.columns and pd.notna(gdf[col].iloc[0]):   end   = str(gdf[col].iloc[0])
except Exception:
    pass  #time filters are not necessary but should be in group_info if so

# query the database only for those users
con = queries.connect(DB_PATH)

#base changesets only for those users in the group
where = ["user IN (" + ",".join(["?"]*len(usernames)) + ")"]
params = usernames[:]

if start:
    where.append("created >= ?")
    params.append(pd.to_datetime(start))
if end:
    where.append("created <= ?")
    params.append(pd.to_datetime(end))

sql = f"""
SELECT
  c.changeset_id AS id,
  c.user,
  c.uid,
  c.created            AS created_at,
  c.comment,
  c.created_by,
  c.imagery_used,
  c.source,
  c.min_lat, c.min_lon, c.max_lat, c.max_lon
FROM changesets c
WHERE {' AND '.join(where)}
"""
changesets = con.execute(sql, params).df()

# attach hashtags for those ids
if not changesets.empty:
    sid = tuple(changesets["id"].tolist())
    tags = con.execute(
        f"SELECT changeset_id AS id, hashtag FROM changeset_hashtags WHERE changeset_id IN ({','.join(['?']*len(sid))})",
        list(sid)
    ).df()
    if not tags.empty:
        h = (tags.groupby("id")["hashtag"]
                 .apply(lambda s: ";".join(sorted({x.strip() for x in s if x and x.strip()})))
                 .reset_index(name="hashtags"))
        changesets = changesets.merge(h, on="id", how="left")
    else:
        changesets["hashtags"] = ""
else:
    # still produce empty files to avoid downstream errors
    changesets = pd.DataFrame(columns=[
        "id","user","uid","created_at","comment","created_by","locale",
        "imagery_used","source","min_lat","min_lon","max_lat","max_lon","hashtags"
    ])

# write overlay files if that was added as an optinoal parameter in running dashboard.qmd
os.makedirs("data/db_overlay", exist_ok=True)

changesets.to_csv("data/db_overlay/changesets_subset.csv", index=False)

# explode hashtags into a tags table
tags_rows = []
for cid, s in zip(changesets["id"], changesets["hashtags"].fillna("")):
    if not s: continue
    for t in [x.strip() for x in s.split(";") if x.strip()]:
        tags_rows.append((cid, "hashtags", t))
pd.DataFrame(tags_rows, columns=["changeset","key","value"]).to_csv(
    "data/db_overlay/changesets_tags_subset.csv", index=False
)

# minimal contributions summary for the group
summary = (changesets.groupby("user", dropna=False)
                        .size()
                        .reset_index(name="map_changesets"))
for col in ["account_age","comments","diary","map_notes","traces","wiki_edits"]:
    summary[col] = 0
summary.to_csv("data/db_overlay/contributions_summary_subset.csv", index=False)

# pairs for the filterable table
pairs = []
for user, s in zip(changesets["user"], changesets["hashtags"].fillna("")):
    if not s: continue
    for t in [x.strip() for x in s.split(";") if x.strip()]:
        pairs.append((user, t))
pd.DataFrame(pairs, columns=["user","hashtag"]).to_csv(
    "data/db_overlay/user_hashtag_pairs.csv", index=False
)

# progress log to verify correct functioning
with open("data/db_overlay/_overlay_log.txt","w") as f:
    f.write(f"Wrote overlay for {len(usernames)} users; "
            f"{len(changesets)} changesets; "
            f"{len(pairs)} user-hashtag pairs.\n")

con.close()
```

```{r load-datasets}
group_info <- read.csv(paste0(base_path, "data/metadata/group_info.csv"))

changesets <- read_csv(paste0(base_path, "data/raw/changesets.csv")) |> 
  mutate(user = as.factor(user))

changesets_sf <- read_sf(paste0(base_path, "data/raw/changesets.gpkg"))

changesets_details <- read_csv(paste0(base_path, "data/raw/changesets_details.csv"))

changesets_tags <- read_csv(paste0(base_path, "data/raw/changesets_tags.csv"))

wiki_contributions <- read.csv(paste0(base_path, "data/raw/wiki_contributions.csv"))

contributions_summary <- read.csv(paste0(base_path, "data/raw/contributions_summary.csv"))

# Optionally we swap the database overlay into some of the variables
# This can be toggled via " -P use_db_overlay:true "

use_overlay <- isTRUE(as.logical(params$use_db_overlay))

if (use_overlay) {
  if (file.exists("data/db_overlay/changesets_subset.csv")) {
    changesets <- readr::read_csv("data/db_overlay/changesets_subset.csv", show_col_types = FALSE)
    if ("user" %in% names(changesets)) changesets$user <- as.factor(changesets$user)
  }
  if (file.exists("data/db_overlay/changesets_tags_subset.csv")) {
    changesets_tags <- readr::read_csv("data/db_overlay/changesets_tags_subset.csv", show_col_types = FALSE)
  }
  if (file.exists("data/db_overlay/contributions_summary_subset.csv")) {
    contributions_summary <- read.csv("data/db_overlay/contributions_summary_subset.csv")
  }
  # We intentionally leave changeset_details and changesets_sf as the planet files as we don't have overlay   equivalents from the database. 
}

```


```{r data-map contributions}

# Uncomment to check if geometry is valid
# st_is_valid(changesets_sf_centroids, reason = TRUE) 

changesets_sf_repaired <- changesets_sf |> 
  st_make_valid() 

changesets_sf_repaired$area_meters <- st_area(changesets_sf_repaired)
changesets$area_meters <- st_area(changesets_sf_repaired)

changesets_sf_centroids <- changesets_sf_repaired |> 
  st_centroid() %>% # Need to use pipe operator to make the below work.
  mutate(lon = sf::st_coordinates(.)[,1],
         lat = sf::st_coordinates(.)[,2])



n_users <- nlevels(changesets$user)

date_start <- dmy(format(min(date(changesets$created_at)), "%d/%m/%Y"))
date_end <- dmy(format(max(date(changesets$created_at)), "%d/%m/%Y"))

n_days <- as.numeric(difftime(date_end, date_start, units = "days"))

contributions_n_changesets <- nrow(changesets)


```

```{r data-wiki-contrib}

wiki_contributions_stats <- calc_stats_contributions_wiki(wiki_contributions)

wiki_contributions_n <- wiki_contributions |> 
  count(user, name = "wiki_edits") |> 
  mutate(user = tolower(user))

```


```{r data-other}

contributions_type <- contributions_summary |> 
  select(-starts_with("date"), -account_age) |> 
  pivot_longer(-user) |> 
  count(name, wt = value) |> 
  filter(name %in% c(
    "comments", 
    "diary", 
    "map_changesets", 
    "map_notes", 
    "traces", 
    "wiki_edits"
    )
  )



```



# Overview

## Title and description 

::: {.card .flow}

## `r group_info$name[1]`

`r group_info$description[1]`

:::


## Value boxes

```{r}
#| content: valuebox
#| title: "# users"
#| icon: people
#| color: primary
nrow(contributions_summary)
```

```{r}
#| content: valuebox
#| title: "Mean account age (years)"
#| icon: hourglass-split
#| color: primary
round(mean(contributions_summary$account_age), digits = 2)
```


Hours spent contributing (estimate based on time x changesets x features + time * written words + time * delete words + time for discussion + time for ...)

## Group Summary (Group Information, Group Description)


::: {.callout-note}
```{r}

describe_group <- function(contribs_tbl = NULL, edits_tbl = NULL, tags_tbl = NULL) {
  #contribution frequency (changesets per day or edits per active day)
  #This computes user contributions per day using changesets across the date range or edits 
  #per active day, labelling < 1 as occasional, 1 - 9 as frequency and >= 10 as heavy.
  #If it can't compute anything it defaults to the description 'active'
  per_day <- NA_real_
  if (!is.null(contribs_tbl) &&
      all(c("changesets","first_edit","last_edit") %in% names(contribs_tbl))) {
    rng  <- range(c(contribs_tbl$first_edit, contribs_tbl$last_edit), na.rm = TRUE)
    days <- as.numeric(diff(rng)) + 1
    total_changesets <- sum(contribs_tbl$changesets, na.rm = TRUE)
    per_day <- ifelse(days > 0, total_changesets / days, total_changesets)
  } else if (!is.null(edits_tbl) && "date" %in% names(edits_tbl)) {
    per_day <- nrow(edits_tbl) / dplyr::n_distinct(edits_tbl$date)
  }
  freq <- dplyr::case_when(
    is.na(per_day)   ~ "active",
    per_day < 1      ~ "occasional",
    per_day < 10     ~ "frequent",
    TRUE             ~ "heavy"
  )

  #activity type (adds / modifies / deletes and creators / repairers / improvers)
  #looks at which action is most common in users. if the most common action is add/create
  #then the user is a creator. if the most common action is modify/update, then the user
  #is a repairer. if the most common action is delete/remove then the user is an improver.
  type_word <- "contributors"
  if (!is.null(edits_tbl) && "action" %in% names(edits_tbl)) {
    top_act <- edits_tbl |> count(action) |> arrange(desc(n)) |> slice(1) |> pull(action)
    type_word <- recode(top_act,
      add = "creators", create = "creators",
      modify = "repairers", update = "repairers",
      delete = "improvers", remove = "improvers",
      .default = "contributors"
    )
  } else if (!is.null(contribs_tbl) && all(c("adds","mods","dels") %in% names(contribs_tbl))) {
    sums <- colSums(contribs_tbl[, c("adds","mods","dels")], na.rm = TRUE)
    type_word <- c(adds = "creators", mods = "repairers", dels = "improvers")[names(which.max(sums))]
  }

  #experience level with average edits per user
  #uses average edits per user (mean(n)) to label users as hobbyists from < 50 edits,
  #pro-ams for 50-499 edits and professionals for >= 500 edits.
  avg_edits <- NA_real_
  if (!is.null(edits_tbl) && "user" %in% names(edits_tbl)) {
    avg_edits <- edits_tbl |> count(user) |> summarise(m = mean(n)) |> pull(m)
  } else if (!is.null(contribs_tbl) && "total_edits" %in% names(contribs_tbl)) {
    avg_edits <- mean(contribs_tbl$total_edits, na.rm = TRUE)
  }
  exp_level <- if (is.na(avg_edits)) "hobbyists" else case_when(
    avg_edits < 50   ~ "hobbyists",
    avg_edits < 500  ~ "pro-ams",
    TRUE             ~ "professionals"
  )

  #user interest areas. Takes the top 3 tag keys from the tags_summary or from the 
  #edit counts. If nothing is available it just says "variety of features"
  top_tags <- character(0)
  if (!is.null(tags_tbl) && all(c("key","n") %in% names(tags_tbl))) {
    top_tags <- tags_tbl |> arrange(desc(n)) |> slice_head(n = 3) |> pull(key)
  } else if (!is.null(edits_tbl) && "key" %in% names(edits_tbl)) {
    top_tags <- edits_tbl |> count(key) |> arrange(desc(n)) |> slice_head(n = 3) |> pull(key)
  }
  tags_text <- if (length(top_tags)) paste(top_tags, collapse = ", ") else "a variety of features"

  #homogeniety using Gini edits per user. If the Gini index is > 0.6 then we label
  #the group as homogenous. If it is between 0.4 to 0.6, then we label the group as somewhat
  #concentrated. If it is <0.4 then the group is diverse. 
  gini <- NA_real_
  if (!is.null(edits_tbl) && "user" %in% names(edits_tbl)) {
    v <- edits_tbl |> count(user) |> pull(n) |> sort()
    if (length(v)) gini <- (2*sum(seq_along(v)*v)/sum(v)/length(v)) - (length(v)+1)/length(v)
  }
  homog <- if (is.na(gini)) "diverse" else if (gini > 0.6) "homogeneous"
           else if (gini > 0.4) "somewhat concentrated" else "diverse"

  #assembling the sentence together
  glue::glue(
    "This is a {homog} group of {freq} {type_word}, mostly {exp_level}, with interests in {tags_text}."
  )
}

# try to use objects that already exist in your dashboard environment
edits_tbl <- if (exists("edits")) edits else if (exists("elements")) elements else NULL
tags_tbl  <- if (exists("tags_summary")) tags_summary else NULL

cat(describe_group(
  contribs_tbl = if (exists("contributions_summary")) contributions_summary else NULL,
  edits_tbl    = edits_tbl,
  tags_tbl     = tags_tbl
))
```
:::

## Some summaries {height=85%}

### User Stats {.tabset}

```{r}
#| title: "Account Age"

hist_account_age <- contributions_summary |>
  ggplot(aes(x = account_age)) +
  geom_histogram(
    breaks = seq(0, 20, by = 2),
    color = colour_primary_mark,
    fill = colour_fill
  ) +
  geom_vline(
    xintercept = mean(contributions_summary$account_age), 
    color = colour_secondary_mark, 
    linetype = 2, 
    lwd = 0.7
  ) + 
  annotate(
    "text", 
    x = mean(contributions_summary$account_age), 
    y = 3, 
    label = paste("Mean:", round(mean(contributions_summary$account_age), 2)),
    color = colour_secondary_mark, 
    fontface = "bold"
  ) + 
  scale_x_continuous(breaks = seq(0, 20, by = 5)) +
  labs(y = "# users", x = "Account's age (in years)") +
  theme_user_stats


```


```{r}
#| title: "Map changesets"

hist_map_changesets <- contributions_summary |>
  ggplot(aes(x = map_changesets)) +
  geom_histogram(
    # breaks = seq(0, max(contributions_summary), by = 2),
    color = colour_primary_mark,
    fill = colour_fill
  ) +
  geom_vline(
    xintercept = mean(contributions_summary$map_changesets), 
    color = colour_secondary_mark, 
    linetype = 2, 
    lwd = 0.7
  ) + 
  annotate(
    "text", 
    x = mean(contributions_summary$map_changesets), 
    y = 3, 
    label = paste("Mean:", round(mean(contributions_summary$map_changesets), 2)),
    color = colour_secondary_mark, 
    fontface = "bold"
  ) + 
  # scale_x_continuous(breaks = seq(0, 20, by = 5)) +
  labs(y = "# of users", x = "Changesets") +
  theme_user_stats

```

```{r}
#| title: "Map changesets"

hist_map_notes <- contributions_summary |>
  ggplot(aes(x = map_notes)) +
  geom_histogram(
    # breaks = seq(0, max(contributions_summary), by = 2),
    color = colour_primary_mark,
    fill = colour_fill
  ) +
  geom_vline(
    xintercept = mean(contributions_summary$map_notes), 
    color = colour_secondary_mark, 
    linetype = 2, 
    lwd = 0.7
  ) + 
  annotate(
    "text", 
    x = mean(contributions_summary$map_notes), 
    y = 3, 
    label = paste("Mean:", round(mean(contributions_summary$map_notes), 2)),
    color = colour_secondary_mark, 
    fontface = "bold"
  ) + 
  # scale_x_continuous(breaks = seq(0, 20, by = 5)) +
  labs(y = "# of users", x = "Notes") +
  theme_user_stats

```

```{r}
#| title: "Comments"

hist_comments <- contributions_summary |>
  ggplot(aes(x = comments)) +
  geom_histogram(
    # breaks = seq(0, max(contributions_summary), by = 2),
    color = colour_primary_mark,
    fill = colour_fill
  ) +
  geom_vline(
    xintercept = mean(contributions_summary$comments), 
    color = colour_secondary_mark, 
    linetype = 2, 
    lwd = 0.7
  ) + 
  annotate(
    "text", 
    x = mean(contributions_summary$comments), 
    y = 3, 
    label = paste("Mean:", round(mean(contributions_summary$comments), 2)),
    color = colour_secondary_mark, 
    fontface = "bold"
  ) + 
  # scale_x_continuous(breaks = seq(0, 20, by = 5)) +
  labs(
    subtitle = "Comments are discussions around a particular changeset",
    x = "Comments",
    y = "# of users"
  ) +
  theme_user_stats

```


```{r}
#| title: "Diaries"

hist_diary <- contributions_summary |>
  ggplot(aes(x = diary)) +
  geom_histogram(
    # breaks = seq(0, max(contributions_summary), by = 2),
    color = colour_primary_mark,
    fill = colour_fill
  ) +
  geom_vline(
    xintercept = mean(contributions_summary$diary), 
    color = colour_secondary_mark, 
    linetype = 2, 
    lwd = 0.7
  ) + 
  annotate(
    "text", 
    x = mean(contributions_summary$diary), 
    y = 3, 
    label = paste("Mean:", round(mean(contributions_summary$diary), 2)),
    color = colour_secondary_mark, 
    fontface = "bold"
  ) + 
  # scale_x_continuous(breaks = seq(0, 20, by = 5)) +
  labs(y = "# of users", x = "Diary entries") +
  theme_user_stats

```


```{r}
#| title: "Traces"

hist_traces <- contributions_summary |>
  ggplot(aes(x = traces)) +
  geom_histogram(
    # breaks = seq(0, max(contributions_summary), by = 2),
    color = colour_primary_mark,
    fill = colour_fill
  ) +
  geom_vline(
    xintercept = mean(contributions_summary$traces), 
    color = colour_secondary_mark, 
    linetype = 2, 
    lwd = 0.7
  ) + 
  annotate(
    "text", 
    x = mean(contributions_summary$traces), 
    y = 3, 
    label = paste("Mean:", round(mean(contributions_summary$traces), 2)),
    color = colour_secondary_mark, 
    fontface = "bold"
  ) + 
  # scale_x_continuous(breaks = seq(0, 20, by = 5)) +
  labs(y = "# of users", x = "Traces") +
  theme_user_stats

```

```{r}
#| title: "Contributions distribution by users"
#| subtitle: "This provides info about how homogeneous or heterogeneous this group is"
#| layout-ncol: 2


ggplotly(hist_account_age) 
ggplotly(hist_map_changesets)
ggplotly(hist_map_notes)
ggplotly(hist_comments)
ggplotly(hist_diary)
ggplotly(hist_traces)

```

```{r}
#| title: "Facets"

contributions_summary |> 
  select(user, account_age, map_changesets, wiki_edits, comments, diary, map_notes, traces) |> 
  pivot_longer(-user) |> 
  mutate(name = as.factor(name),
         name = fct_relevel(name, "account_age", "map_changesets", "wiki_edits", "comments", "diary", "map_notes", "traces")) |> 
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(
    # breaks = seq(0, max(contributions_summary), by = 2),
    # color = colour_primary_mark,
    # fill = colour_fill
  ) +
  geom_vline(
    xintercept = mean(~value), 
    # color = colour_secondary_mark, 
    linetype = 2, 
    lwd = 0.7
  ) + 
  facet_wrap(~name, ncol = 2, scales = "free_x")+ 
  labs(y = "", x ="") +
  # theme_minimal() +
  theme(
    axis.title.y=element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    panel.grid= element_blank(),
    legend.position = "none"
  )


```


### Contributions distribution

```{r}
#| title: "Contributions per type"

contributions_type |> 
  plot_ly(labels = ~name, values = ~n) |> 
  add_pie(hole = 0.6)

```


# Changesets {icon="fa-utensils"}

## Valueboxes

```{r}
#| content: valuebox
#| title: "# users with changesets"
#| icon: map
#| color: primary
contributions_summary |> 
  filter(map_changesets > 0) |> 
  nrow()
```

```{r}
#| content: valuebox
#| title: "# changesets"
#| icon: pencil
#| color: secondary
contributions_n_changesets
```

```{r}
#| content: valuebox
#| title: "changesets/day"
#| icon: pencil-fill
round(nrow(changesets)/n_days, digits = 2)
```

<!-- Create a boxplot that shows on the x axis number of changesets, on the Y axis number of features per changeset, and the size of the dot can be the area of the changeset. Do this for every member of the group. -->


## Map of contributions { height="300"}

```{r}
#| title: Where do they contribute?
#| padding: 0px
leaflet(options = leafletOptions(maxZoom = 12)) |> 
  addTiles() |>   # Add default OpenStreetMap map tiles
  addProviderTiles(providers$CartoDB.Positron) |> 
  # addProviderTiles(providers$Esri.WorldStreetMap) |> 
  # addPolygons(data = changesets_sf) |> 
  # addCircles(data = changesets_sf_centroids)
  addHeatmap(data = filter(changesets_sf_centroids, !is.na(lon)), lng = ~lon, lat = ~lat,
           blur = 10, radius = 20, minOpacity = 0.5 ) 
```


```{r}
#| title: When do they contribute?

# Modified - OSM_Aug_RA
# changesets_day_time <- changesets |> 
#   select(created_at) |> 
#   mutate(date = created_at,
#          weekday = weekdays(date),
#          weekday = forcats::fct_relevel(
#            as.factor(weekday), 
#            "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"),
#          time = hms::as_hms(date),
#          hour = hour(time)) |> 
#   count(weekday, hour) |> 
#   # Converting to wide to generate NAs
#   pivot_wider(names_from = hour, values_from = n) |>
#   pivot_longer(-weekday, names_to = "hour", values_to = "n") |> 
#   mutate(hour = as.numeric(hour)) 
# # mutate(n = replace_na(n, 0))
# 
# heatmap <- ggplot(changesets_day_time, aes(hour, weekday, fill= n)) + 
#   geom_tile(show.legend = FALSE) +
#   labs(y = NULL) + 
#   scale_fill_distiller(palette = "Greens", direction = 1) +
#   # scale_x_continuous(position = "top")  + # Does not work with plotly!
#   theme_minimal() 
# # theme(panel.background = element_rect(fill = 'lightgrey'),
# #       # panel.grid.minor = element_line(color = 'white', size = 2)
# #       )
# 
# heatmap_interactive <- ggplotly(heatmap)
# 
# heatmap_interactive

changesets_day_time <- changesets |>
  select(created_at) |>
  mutate(
    date = created_at,
    month = factor(as.integer(format(date, "%m")), levels = 1:12, labels = month.abb, ordered = TRUE),
    weekday = weekdays(date),
    weekday = forcats::fct_relevel(
      as.factor(weekday),
      "Monday",
      "Tuesday",
      "Wednesday",
      "Thursday",
      "Friday",
      "Saturday",
      "Sunday"
    )
  ) |>   count(month, weekday)

changeset_heatmap <-
  ggplot(changesets_day_time, aes(month, weekday, fill = n)) +
  geom_tile() +
  labs(x = NULL, y = NULL) +
  scale_fill_gradient2(
    low = "#ffffff",
    mid = "#d8daeb",
    high = "#2d004b",
    name = "Number of Changes"
  ) +
  scale_y_discrete(limits = rev) +
  #coord_fixed() +
  theme_heatmap

changeset_heatmap_interactive <- ggplotly(changeset_heatmap)

changeset_heatmap_interactive

```


### {.tabset}

```{r}
#| title: "Changesets and size"
hist_changesets_size <- changesets_details |> 
  count(changeset, name = "n_features") |>
  mutate(changeset = as.numeric(changeset)) |> 
  right_join(changesets, by = join_by(changeset == id)) |> 
  select(changeset, n_features, area_meters) |> 
  mutate(area_hectare = area_meters * 0.0001,
         area_hectare = as.numeric(area_hectare)) |> 
  select(n_features, area_hectare) |> 
  pivot_longer(c(n_features, area_hectare)) |> 
  ggplot(aes(x = value, fill = name)) +
  geom_histogram() +
  facet_wrap(~name, ncol=1, scales = "free_x")+ 
  labs(y = "", x = "") +
  theme_minimal() +
  theme(
    axis.title.y=element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    panel.grid= element_blank()
  ) 

ggplotly(hist_changesets_size)


```

## Row

### What {.tabset}

```{r}
#| title: "Most Used keys"
key_counts <- changesets_tags  |>
  count(key, sort = TRUE) |> 
  categorise_keys() |> 
  mutate(parent_key = replace_na(parent_key, "other"),
         top_key = replace_na(top_key, "other"))

top_keys <- levels(as.factor(key_counts$top_key))
top_keys_n <- nlevels(as.factor(key_counts$top_key))


tags_top_parents <- data.frame(
  key = top_keys,
  n = rep(0, top_keys_n),
  top_key = rep("", top_keys_n) 
)

key_counts <- key_counts |> 
  bind_rows(tags_top_parents) |> 
  filter(key != "leisure")

plot_ly(
  data = key_counts,
  type = "treemap",
  labels = ~key,
  values = ~n,
  parents = ~top_key,
  hoverinfo = "label+value+percent parent+percent entry+percent root"
)

```


```{r}
#| title: "Hashtags used"
#| label: hashtags-count2
# TODO: see how to reuse code.

# Modified - OSM_Aug_RA
# changesets |> 
#   select(id, hashtags) |> 
#   filter(!is.na(hashtags)) |> 
#   # mutate(hashtags = str_remove_all(hashtags, "#")) |> 
#   separate_longer_delim(hashtags, ";") |> 
#   count(hashtags) |> 
#   mutate(hashtags = fct_reorder(hashtags, n),
#          hashtags = fct_lump_n(hashtags, 5)) |> 
#   ggplot(aes(x = n, y = hashtags, label = n)) +
#   geom_bar(stat="identity") +
#   geom_text(color = "white", hjust = 1.2) +
#   labs(x = NULL, y = NULL) +
#   theme_minimal() +
#   theme(panel.grid= element_blank(),
#         axis.text.x=element_blank(),
#         axis.ticks.x = element_blank())

set.seed(42)

# wordcloud2 displays words randomly. Most frequent words tend to get
# missed out. Sorting data in decreasing order of word frequency to avoid this.
# https://stackoverflow.com/questions/41654007/r-wordcloud2-does-not-always-render-the-most-frequent-words
word_corpus <- changesets |>
  select(id, hashtags) |>
  filter(!is.na(hashtags)) |>
  # mutate(hashtags = str_remove_all(hashtags, "#")) |>
  separate_longer_delim(hashtags, ";") |>
  count(hashtags) |>
  mutate(hashtags = fct_reorder(hashtags, n),
         hashtags = fct_lump_n(hashtags, 5)) |>
  arrange(desc(n))

# wordcloud2 does not work well with Quarto or Markdown
# wordcloud2::wordcloud2(
#   data = word_corpus,
#   minRotation = 0,
#   maxRotation = 0,
#   size = .3,
#   fontFamily = "Arial",
#   backgroundColor = "white",
#   color = rep_len(c(colour_primary_mark, colour_tertiary_mark), length.out = nrow(word_corpus))
# )

# smaller scale helps with printing all words in the corpus.
wordcloud(words = word_corpus$hashtags, 
          freq = word_corpus$n, 
          min.freq = 1, 
          random.order=FALSE, 
          colors=brewer.pal(4, "PuOr"),
          scale=c(2,0.5)) 
```



```{r}
#| title: "Comments"
library(tidytext)

changesets |>
  select(comment) |> 
  unnest_tokens(bigram, comment, token = "ngrams", n = 2) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1),
         !is.na(word2)) |> 
  mutate(bigram = paste(word1, word2)) |> 
  count(bigram, sort = TRUE) |> 
  head(15) |> 
  kable()
```



```{r}
#| title: "Most Used keys 2"
key_counts2 <- changesets_tags  |>
  count(key, sort = TRUE) |> 
  categorise_keys() |> 
  count(top_key, parent_key, wt = n) |> 
  mutate(parent_key = replace_na(parent_key, "other"),
         top_key = replace_na(top_key, "other"),
         top_key = fct_infreq(as.factor(top_key))) 


tags_treemap <- ggplot(key_counts2, aes(area = n, fill = top_key, 
                        subgroup = top_key,
                        label = parent_key)) +
  geom_treemap(color = "white") +
  geom_treemap_subgroup_border(color = "white") +
  geom_treemap_text(fontface = "italic", colour = "white", alpha = 0.5,
                    place = "centre",
                    grow = TRUE) +
  geom_treemap_subgroup_text(place = "bottom", grow = T, alpha = 1, colour =
                               "#333333", fontface = "italic", min.size = 0) +
  labs(fill = "") +
  scale_fill_brewer(palette = "Paired", type = "qual", direction = 1) +
  # TODO: sort sorting for scale fill. Now based on factor's levels.
  theme(legend.position="none")

tags_treemap

```


### How {.tabset}


```{r}
#| title: "Sources of information"

df <- changesets |> 
  select(imagery_used, source) |> 
  mutate(
    source = case_when(
      is.na(source) & !is.na(imagery_used) ~ imagery_used,
      .default = source)) |>
  filter(!is.na(source)) |> 
  separate_wider_delim(source, delim = ";", names_sep = "_", too_few = "align_start", cols_remove = TRUE) |> 
  select(starts_with("source_")) |> 
  pivot_longer(everything()) |> 
  select(value) |> 
  rename(source = value) |> 
  filter(!is.na(source)) |> 
  mutate(source = str_trim(source),
         source = str_replace_all(source, "\\+", " "),
         source = case_when(
           tolower(source) == "survey" ~ "Survey",
           str_detect(tolower(source), "mapillary") ~ "Mapillary",
           str_detect(tolower(source), "esri")   ~ "Esri World Imagery",
           str_detect(source, "Bing") ~ "Bing Maps Aerial",
           str_detect(source, "PNOA") ~ "PNOA Spain",
           str_detect(source, "Maxar") ~ "Maxar Premium Imagery",
           str_detect(source, "Catastro") ~ "Catastro Spain",
           str_detect(source, "Strava") ~ "Strava Heat Map",
           .default = source
         ),
         parent = case_when(
           str_detect(source, "Bing") ~ "Satellite Imagery",
           str_detect(source, "Copernicus") ~ "Satellite Imagery",
           str_detect(source, "Esri") ~ "Satellite Imagery",
           str_detect(tolower(source), "imagery") ~ "Satellite Imagery",
           source %in% c("PNOA Spain") ~ "Satellite Imagery"
         )) |> 
  mutate(source = as.factor(source),
         source = fct_lump_n(source, 10)) |> 
  count(source, parent, sort = TRUE) 


top_parents <- levels(df$parents)
top_parents_n <- nlevels(df$parents)

df_top_parents <- data.frame(
  source = "Satellite Imagery",
  n = 0,
  parent = ""
)

df <- df |> 
  bind_rows(df_top_parents) 


plot_ly(
  data = df,
  type = "sunburst",
  # branchvalues = 'total',
  labels = ~source,
  values = ~n,
  parents = ~parent
)

```


```{r}
#| title: "Software used"

changesets |> 
  select(created_by) |> 
  rename(software = created_by) |> 
  mutate(software_clean = case_when(
    str_detect(software, "iD ") ~ "iD",
    str_detect(software, "JOSM") ~ "JOSM",
    str_detect(software, "maps.me") ~ "maps.me",
    str_detect(software, "MapComplete") ~ "MapComplete",
    str_detect(software, "Organic Maps") ~ "Organic Maps",
    str_detect(software, "RapiD") ~ "RapiD",
    str_detect(software, "StreetComplete") ~ "StreetComplete",
    str_detect(software, "Vespucci") ~ "Vespucci",
    .default = "Other"
  )) |> 
  count(software_clean, sort = TRUE) |> 
  mutate(software_clean = fct_reorder(software_clean, n)) |> 
  plot_ly(labels = ~software_clean, values = ~n) |> 
  add_pie(hole = 0.6)


```

```{r}
#| title: "Type of changesets"
changesets_tags |> 
  count(action_type, sort = TRUE) |> 
  plot_ly(labels = ~action_type, values = ~n) |> 
  add_pie(hole = 0.6)
```

```{r}
#| title: Language

changesets |> 
  select(locale) |> 
  separate_wider_delim(
    locale, delim = "-", names = c("language", "country"), 
    too_few = "align_start",
    cols_remove = FALSE
  ) |> 
  filter(!is.na(language)) |> 
  count(language) |> 
  mutate(language = fct_reorder(language, n)) |> 
  plot_ly(labels = ~language, values = ~n) |> 
  add_pie(hole = 0.6)

```






# Wiki
```{r}
wiki_stats <- calc_stats_contributions_wiki(wiki_contributions) 
```

## Valueboxes

```{r}
#| content: valuebox
#| title: "# users with wiki edits"
#| icon: wikipedia
#| color: primary
wiki_stats[1]
```

```{r}
#| content: valuebox
#| title: "total additions"
#| icon: database-add
#| color: green

df <- as.data.frame(wiki_stats[4])
round(sum(df$additions, na.rm = TRUE), digits = 2)

  
```

```{r}
#| content: valuebox
#| title: "total deletions"
#| icon: database-dash
#| color: red

df <- as.data.frame(wiki_stats[4])
round(sum(df$deletions, na.rm = TRUE), digits = 2)

```

```{r}
#| content: valuebox
#| title: "# of new pages created"
#| icon: file-earmark-plus

# sum(wiki_contributions$new, na.rm = TRUE)
wiki_contributions |> 
  filter(parentid == 0) |> 
  count(parentid) |> 
  pull(n)


```


## Row


```{r}
#| title: When do they contribute?

# Modified - OSM_Aug_RA
# wiki_edits_day_time <- wiki_contributions |> 
#   select(timestamp) |> 
#   mutate(date = ymd_hms(timestamp),
#          weekday = weekdays(date),
#          weekday = forcats::fct_relevel(
#            as.factor(weekday), 
#            "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"),
#          time = hms::as_hms(date),
#          hour = hour(time)) |> 
#   count(weekday, hour) |> 
#   # Converting to wide to generate NAs
#   pivot_wider(names_from = hour, values_from = n) |>
#   pivot_longer(-weekday, names_to = "hour", values_to = "n") |> 
#   mutate(hour = as.numeric(hour)) 
# # mutate(n = replace_na(n, 0))
# 
# heatmap <- ggplot(wiki_edits_day_time, aes(hour, weekday, fill= n)) + 
#   geom_tile(show.legend = FALSE) +
#   labs(y = NULL) + 
#   scale_fill_distiller(palette = "Blues", direction = 1) +
#   # scale_x_continuous(position = "top")  + # Does not work with plotly!
#   theme_minimal() 
# # theme(panel.background = element_rect(fill = 'lightgrey'),
# #       # panel.grid.minor = element_line(color = 'white', size = 2)
# #       )
# 
# heatmap_interactive <- ggplotly(heatmap)
# 
# heatmap_interactive

wiki_edits_day_time <- wiki_contributions |>
  select(timestamp) |>
  mutate(
    date = ymd_hms(timestamp),
    month = factor(month(date), levels = c(1:12), labels = month.abb),
    weekday = weekdays(date),
    weekday = forcats::fct_relevel(
      as.factor(weekday),
      "Monday",
      "Tuesday",
      "Wednesday",
      "Thursday",
      "Friday",
      "Saturday",
      "Sunday"
    )
  ) |>   
  count(month, weekday)


# plotly.js does not (yet) support horizontal legend items, hence legend is
# vertical on the side
wiki_heatmap <-
  ggplot(wiki_edits_day_time, aes(month, weekday, fill = n)) +
  geom_tile() +
  # subtitle not working as expected
  labs(x = NULL, y = NULL) +
  scale_fill_gradient2(
    low = "#ffffff",
    mid = "#d8daeb",
    high = "#2d004b",
    name = "Number of Edits"
  ) +
  scale_y_discrete(limits = rev) +
  #coord_fixed() +
  theme_heatmap

wiki_heatmap_interactive <- ggplotly(wiki_heatmap)

wiki_heatmap_interactive

```

```{r}
#| title: "Written words by language"
wiki_contributions |> 
  select(title, sizediff) |> 
  mutate(lang = str_extract(title, "^([a-zA-Z]){2}\\:"),
         lang = str_remove(lang, ":"),
         lang = str_to_upper(lang),
         lang = case_when(is.na(lang) ~ "EN",
                          .default = lang)) |> 
  count(lang, wt = sizediff, sort = TRUE) |> 
  mutate(lang = fct_reorder(lang, n)) |> 
  plot_ly(labels = ~lang, values = ~n) |> 
  add_pie(hole = 0.6)
```

```{r}
#| title: "Wiki changes and size"
p <- wiki_contributions |> 
  select(size, sizediff) |> 
  # mutate(sizediff = abs(sizediff)) |> 
  ggplot(aes(x = size, y = sizediff)) +
  geom_point() +
  # scale_x_log10() +
  # scale_y_log10() + 
  labs(x = "Page size",
       y = "Characters added/remov") + 
  theme_minimal()

ggplotly(p)
```


## Row
```{r}
#| title: "Top 15 contributed wiki pages"
plot <- wiki_contributions_stats[[5]] |>
  mutate(title = fct_reorder(title, n)) |> 
  head(15) |> 
  ggplot(aes(x = n, y = title, label = n)) +
  geom_bar(stat="identity") +
  # geom_text(color = "white", hjust = 1.2) +
  labs(x = NULL, y = NULL) +
  theme_minimal() +
  theme(panel.grid= element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x = element_blank())
 

ggplotly(plot)

```

# Other contributions {orientation="columns"}

## Diaries
```{r}
#| content: valuebox
#| title: "# users with diary entries"
#| icon: pencil-square
#| color: primary
#| height: 30px
contributions_summary |> 
  filter(diary > 0) |> 
  nrow()
```

Total diary entries

Wordcount based on titles

## Notes

```{r}
#| content: valuebox
#| title: "# users with map notes"
#| icon: sticky
#| color: primary
notes_users <- contributions_summary |> 
  filter(map_notes > 0) |> 
  nrow()

notes_users
```

```{r}
#| content: valuebox
#| title: "# of notes"
#| icon: sticky
#| color: primary
notes_total <- contributions_summary |> 
  select(map_notes) |> 
  pivot_longer(map_notes) |> 
  count(name, wt = value) |> 
  pull(n)

format(notes_total, big.mark = ",")
```

```{r}
#| content: valuebox
#| title: "# of notes/user"
#| icon: sticky
#| color: primary
notes_total / notes_users
```

Wordcout based on notes contents

WordLength by notes?


## Traces

```{r}
#| content: valuebox
#| title: "# users with gpx traces"
#| icon: pin-map
#| color: primary
contributions_summary |> 
  filter(traces > 0) |> 
  nrow()
```

### 

Total km

### 
Total traces by country

# About {scrolling="true"}

::: {.card .flow}


This dashboard is a proof of concept that displays how a group of users contribute to OpenStreetMap with special consideration to EDI issues. That means, for example, that this dashboard recognises different types of contributions and all data shown has been anonymised.

This dashboard is a result of a codesign process with members of [Geochicas](https://wiki.openstreetmap.org/wiki/LatAm/Groups/GeoChicas) and the [Centre for Interdisciplinary Methodologies](https://warwick.ac.uk/cim) at the University of Warwick, and is inspired by Pascal Neis' great [HDYC](https://hdyc.neis-one.org/), but with notable important differences: 

- It displays how certain groups of users contribute to OSM, without revealing who their members are. For the purposes of this dashboard, a group is a number of users that have a shared interests who self-report as members of a group.
- It recognises that there are different ways of contributing to OSM beyond map contributions (e.g. wiki edits, user diaries...). As such, it incorporates data from different sources (see below).
- It is opensource: you can see the code (and contribute to it!) in [this GitHub repository](https://github.com/WarwickCIM/OSMdashboard).
- It is self-hosted (for now! -keep tuned), meaning that data stays in your computer.
- It is way slower! 

## Data Sources

- OpenStreetMap API (via [`{{osmapiR}})`](https://docs.ropensci.org/osmapiR/index.html))
- OpenStreetMap Wiki API 
- OpenStreetMap User profiles

## Acknowledgements

## How to cite

If you want to cite this work, please do it as follows:

:::

